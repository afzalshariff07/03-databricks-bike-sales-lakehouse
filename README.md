# Enterprise-Grade Databricks Lakehouse Implementation

## Overview

This repository demonstrates a production-style **Data Lakehouse architecture** implemented on Databricks using the Medallion (Bronzeâ€“Silverâ€“Gold) framework.

The project simulates CRM and ERP source systems integrated into a unified analytical model to support Sales and Customer Analytics use cases.

It covers the full lifecycle:

- Data ingestion
- Data transformation & cleansing
- Dimensional modeling
- Pipeline orchestration
- Performance optimization

---

## Project Objective

This project demonstrates the design and implementation of a complete **Data Lakehouse architecture** using Databricks and the Medallion (Bronzeâ€“Silverâ€“Gold) framework.

The objective is to simulate how modern data platforms are built in enterprise environments â€” starting from raw source data and progressively transforming it into clean, validated, analytics-ready datasets.

The implementation includes:

- End-to-end ingestion of CRM and ERP datasets
- Structured Medallion architecture design
- Data quality enforcement and validation
- Dimensional modeling using a star schema
- Notebook orchestration and automation
- Delta Lake optimization techniques

This implementation reflects best practices used in enterprise data engineering teams for building scalable and maintainable Lakehouse platforms.

---

## Business Scenario

Organizations often operate multiple systems (CRM, ERP, operational databases).  
This project demonstrates how raw operational data can be consolidated and transformed into analytics-ready fact and dimension tables using modern Lakehouse architecture.

The final Gold layer supports:

- Sales performance analysis
- Customer segmentation
- Product analytics
- Business KPI reporting
- Executive dashboard consumption

---

## Architecture

This project follows the **Medallion Architecture** pattern.

### ðŸ¥‰ Bronze Layer â€“ Raw Ingestion

- Ingest raw CRM and ERP datasets
- Store data in Delta format
- Maintain schema consistency
- Support incremental ingestion
- Preserve source-level granularity

### ðŸ¥ˆ Silver Layer â€“ Data Standardization

- Data cleansing and validation
- Type casting and schema enforcement
- Deduplication and business rule application
- Creation of standardized, conformed datasets

### ðŸ¥‡ Gold Layer â€“ Business Transformation

- Star schema dimensional modeling
- Creation of fact and dimension tables
- Aggregated KPI-ready datasets
- Optimized tables for BI and analytics consumption

---

## Project Structure

```
03-DATABRICKS-BIKE-SALES-LAKEHOUSE/
â”‚
â”œâ”€â”€ docs/ # Documentation and supporting material
â”‚
â”œâ”€â”€ datasets/ # Raw source datasets
â”‚ â”œâ”€â”€ source_crm/
â”‚ â”‚ â”œâ”€â”€ cust_info.csv
â”‚ â”‚ â”œâ”€â”€ prd_info.csv
â”‚ â”‚ â””â”€â”€ sales_details.csv
â”‚ â”‚
â”‚ â””â”€â”€ source_erp/
â”‚ â”œâ”€â”€ CUST_AZ12.csv
â”‚ â”œâ”€â”€ LOC_A101.csv
â”‚ â””â”€â”€ PX_CAT_G1V2.csv
â”‚
â”œâ”€â”€ script/ # Databricks notebooks (Bronze/Silver/Gold)
â”‚ â”œâ”€â”€ bronze/
â”‚ â”‚ â”œâ”€â”€ bronze_crm_ingestion.ipynb
â”‚ â”‚ â”œâ”€â”€ bronze_erp_ingestion.ipynb
â”‚ â”‚
â”‚ â”œâ”€â”€ silver/
â”‚ â”‚ â”œâ”€â”€ silver_crm_customers.ipynb
â”‚ â”‚ â”œâ”€â”€ silver_erp_products.ipynb
â”‚ â”‚ â”œâ”€â”€ silver_orchestration.ipynb
â”‚ â”‚
â”‚ â”œâ”€â”€ gold/
â”‚ â”‚ â”œâ”€â”€ gold_dim_customers.ipynb
â”‚ â”‚ â”œâ”€â”€ gold_dim_products.ipynb
â”‚ â”‚ â”œâ”€â”€ gold_fact_sales.ipynb
â”‚ â”‚ â”œâ”€â”€ gold_orchestration.ipynb
â”‚ â”‚
â”‚ â””â”€â”€ init_lakehouse.ipynb
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md

```

---

## Technologies Used

- Databricks
- Apache Spark
- PySpark
- Spark SQL
- Delta Lake
- Unity Catalog

---

## Key Concepts Demonstrated

- Medallion Architecture
- Delta Lake table management
- Incremental data processing
- Data quality enforcement
- Dimensional modeling (Star Schema)
- Notebook orchestration
- Lakehouse optimization techniques (partitioning, optimization)

---

## How to Run

1. Initialize environment using `init_lakehouse.ipynb`
2. Execute Bronze layer notebooks
3. Run Silver transformations
4. Execute Gold layer notebooks
5. Validate final fact and dimension outputs

---

## Acknowledgment

This project draws conceptual inspiration from the Databricks Lakehouse Bootcamp by **Baraa Khatib Salkini**.

The architecture, notebook structure, and enhancements in this repository were independently implemented and adapted to simulate enterprise-level data engineering practices.

---

## Author

Mohammed Afzal Shariff  
BI Associate Manager | Data & Analytics Professional  
Bengaluru, India
